{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Quiz\n",
    "\n",
    "## Chenxi Jiang - cj2706\n",
    "\n",
    "### Due Tues. Nov 15th 11:59pm\n",
    "\n",
    "In this quiz, we're going to load documents from 2 categories (space, cars) in the 20newsgroups dataset. The goal is to train a classifier that classifies documents into these 2 categories based on a term frequency representation of the documents. We will then calculate mean cross-validaion accuracy of a RandomForestClassifier using this transformation.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Replace the Name and UNI in cell above and the notebook filename\n",
    "\n",
    "Replace all '____' below using the instructions provided.\n",
    "\n",
    "When completed, \n",
    " 1. make sure you've replaced Name and UNI in the first cell and filename\n",
    " 2. Kernel -> Restart & Run All to run all cells in order \n",
    " 3. Print Preview -> Print (Landscape Layout) -> Save to pdf \n",
    " 4. post pdf to GradeScope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset using fetch_20newsgroups().\n",
    "#   Only fetch the two categories of interest using categories=['sci.space','rec.autos']\n",
    "# Store in the result into newsgroups\n",
    "newsgroups = fetch_20newsgroups(categories=['sci.space','rec.autos'])\n",
    "\n",
    "# Store the newsgroups.data as docs, newsgroups.target as y and newsgroups.target_names as y_names\n",
    "docs = newsgroups.data\n",
    "y = newsgroups.target\n",
    "y_names = newsgroups.target_names\n",
    "\n",
    "# Print the number of observations by printing the length of docs\n",
    "#  You should get 1187\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: prb@access.digex.com (Pat)\n",
      "Subject: Re: Proton/Centaur?\n",
      "Organization: Express Access Online Communications USA\n",
      "Lines: 15\n",
      "NNTP-Posting-Host: access.digex.net\n",
      "\n",
      "\n",
      "Well thank you dennis for your as usual highly detailed and informative \n",
      "posting.   \n",
      "\n",
      "The question i have about the proton, is  could it be  handled at\n",
      "one of KSC's spare pads, without major  malfunction,  or could it be\n",
      "handled at kourou  or Vandenberg?   \n",
      "\n",
      "Now if it uses storables,  then  how long would it take for the russians\n",
      "to equip something at cape york?\n",
      "\n",
      "If  Proton were launched from a western site,  how would it compare to the\n",
      "T4/centaur?   As i see it, it should lift  very close to the T4.\n",
      "\n",
      "pat\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"From: prb@access.digex.com (Pat)\\nSubject: Re: Proton/Centaur?\\nOrganization: Express Access Online Communications USA\\nLines: 15\\nNNTP-Posting-Host: access.digex.net\\n\\n\\nWell thank you dennis for your as usual highly detailed and informative \\nposting.   \\n\\nThe question i have about the proton, is  could it be  handled at\\none of KSC's spare pads, without major  malfunction,  or could it be\\nhandled at kourou  or Vandenberg?   \\n\\nNow if it uses storables,  then  how long would it take for the russians\\nto equip something at cape york?\\n\\nIf  Proton were launched from a western site,  how would it compare to the\\nT4/centaur?   As i see it, it should lift  very close to the T4.\\n\\npat\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the text of the first document in docs.\n",
    "# Note: try printing both with and without the print() statement\n",
    "#  with the print statement, linebreaks are parsed,\n",
    "#  without, linebreaks are printed as excape characters\n",
    "print(docs[0])\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Print the target value of the first document in y.\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "# Print the target_name of the first document using y_names and y.\n",
    "print(y_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer to Convert To TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1187, 5893)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a CountVectorizer object. It should\n",
    "#   lowercase all text, \n",
    "#   include both unigrams and bigrams: ngram_range=(1,2)\n",
    "#   exclude terms that occur in fewer than 10 documents: min_df=10\n",
    "#   exclude terms that occur in more than 95% of documents: max_df=.95\n",
    "# Store as cvect\n",
    "cvect = CountVectorizer(lowercase = True, # default, transform all docs to lowercase\n",
    "                        ngram_range=(1,2), \n",
    "                        min_df=10, # default, keep all terms\n",
    "                        max_df=.95)\n",
    "\n",
    "# Fit cvect on docs and transform docs into their term frequency representation.\n",
    "# Store as X_tf\n",
    "X_tf = cvect.fit_transform(docs)\n",
    "\n",
    "# Print the shape of X_tf. \n",
    "# The number of rows should match the number of documents above\n",
    "#  and the number of columns should be near 6000\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justina-irene/opt/anaconda3/envs/eods-f22/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['zoo', 'zoo toronto', 'zoology', 'zoology kipling', 'zoology lines']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the last 5 terms in the learned vocabulary in cvect\n",
    "#   using .get_feature_names_out() which returns a list of terms corresponding\n",
    "#   to the order of the columns in X_tf\n",
    "# They should all be related to zoos or zoology\n",
    "cvect.get_feature_names()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['them over', 'getting caught', 'seawater', 'the sprayed', '__ no']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The stopwords learned by cvect are stored as a set in cvect.stop_words_\n",
    "# We'd like to print out a small subset of these terms.\n",
    "# One way to get a subset of a set is to treat it as a list.\n",
    "# First, convert the stop_words_ set to a list.\n",
    "# Store as stop_words_list\n",
    "stop_words_list = list(cvect.stop_words_)\n",
    "\n",
    "# Print out the first 5 elements in stop_words_list.\n",
    "# Note that, since a set is unordered, \n",
    "#  there is no meaning to the ordering of these terms and they may vary over runs.\n",
    "stop_words_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean CV Accuracy Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf cv accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Import cross_val_score from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Import RandomForestClassifier from sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Get a set of 5-fold CV scores using\n",
    "#  a RandomForestClassifier \n",
    "#   with 50 trees \n",
    "#   and n_jobs=-1 all other settings default\n",
    "#   and the full dataset X_tf and y\n",
    "# Store as cv_scores\n",
    "cv_scores = cross_val_score(RandomForestClassifier(n_estimators = 50),X_tf,y,\n",
    "                            cv = 5,\n",
    "                           n_jobs =-1)\n",
    "\n",
    "# Print the mean of these cv_scores rounded to a precision of 2.\n",
    "#  The mean accuracy should be above .9\n",
    "print(f'rf cv accuracy: {cv_scores.mean().round(2):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Find Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer stores the feature names (terms in the vocabulary) in two ways:\n",
    "#  1. as a dictionary of term:colum_index pairs, accessed via cvect.vocabulary_\n",
    "#  2. as a list of terms, in column index order, accessed via cvect.get_feature_names_out()\n",
    "#\n",
    "# We can get the indices of the most important features by training a new RandomForestClassifier on X_tf,y\n",
    "#  and accessing .feature_importances_\n",
    "#\n",
    "# Using some combination of the above data-structures, \n",
    "#  print out the top 10 terms in the vocabulary\n",
    "#  ranked by the feature importances learned by a RandomForestClassifier with 50 trees\n",
    "# \n",
    "# The terms you find will likely not be surprising given the document categories.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f22",
   "language": "python",
   "name": "eods-f22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
